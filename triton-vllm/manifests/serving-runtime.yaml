apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-runtime
spec:
  containers:
    - args:
        - tritonserver
        - "--model-repository=/mnt/models"
        - "--model-control-mode=explicit"
        - "--http-port=8080"
      image: nvcr.io/nvidia/tritonserver:24.08-vllm-python-py3
      name: kserve-container
      ports:
        - name: http-port
          containerPort: 8080  # GRPC port for the inference server
        - name: grcp-port
          containerPort: 8001  # GRPC port for the inference server
        - name: metrics 
          containerPort: 8002  # Metrics port for the inference server
      env:
        - name: HF_HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
        - name: HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
        - name: NCCL_DEBUG
          value: TRACE
        - name: NCCL_IGNORE_DISABLED_P2P
          value: '1'
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - autoSelect: true
      name: transformers
      version: latest
  volumes:
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 2Gi 