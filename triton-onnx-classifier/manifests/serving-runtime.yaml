apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  name: triton-direct-s3-runtime
spec:
  containers:
    - args:
      - tritonserver
      - "--model-repository=/mnt/models"
      - "--model-control-mode=explicit"
      - "--http-port=8080"
      image: nvcr.io/nvidia/tritonserver:24.08-py3
      name: kserve-container
      ports:
        - name: http-port
          containerPort: 8080  # HTTP port for the inference server
        - name: grpc-port
          containerPort: 8001  # GRPC port for the inference server
        - name: metrics 
          containerPort: 8002  # Metrics port for the inference server
      env:
        - name: HF_HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
        - name: HOME
          value: /opt/tritonserver/caches # Path to the writable cache (For model storage)
  multiModel: false
  protocolVersions:
    - v2
    - grpc-v2
  supportedModelFormats:
    - name: triton
      version: latest

